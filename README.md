# ğŸš€ Pipeline Batch Bovespa

Este projeto implementa um **pipeline de dados completo** para **extraÃ§Ã£o, processamento e anÃ¡lise de dados do pregÃ£o da B3**, utilizando **AWS S3, AWS Lambda, AWS Glue e AWS Athena**.

## ğŸ“Œ Objetivo

Criar um fluxo de **ingestÃ£o e processamento de dados da B3**, permitindo que os dados brutos sejam extraÃ­dos, transformados e armazenados em um formato otimizado para anÃ¡lise no **Athena**.

## ğŸ“ Arquitetura

O pipeline segue a arquitetura abaixo:

1. **Scraping dos dados da B3**: Um script Python coleta os dados do pregÃ£o em um job executado sempre as 10h.
2. **Armazenamento no S3 (Raw Bucket)**: Os dados brutos sÃ£o salvos no S3 em formato Parquet, com partiÃ§Ã£o diÃ¡ria.
3. **Trigger Lambda**: Ao detectar um novo arquivo no S3, uma Lambda Ã© acionada.
4. **ETL com AWS Glue**: A Lambda inicia um job no Glue para processar os dados.
5. **Armazenamento dos dados refinados**: O Glue salva os dados no S3 (Refined Bucket), particionando por data e nome da aÃ§Ã£o.
6. **Consulta via AWS Athena**: Os dados sÃ£o catalogados e podem ser consultados no Athena.

![img_1.png](img_1.png)

## ğŸ›  Tecnologias Utilizadas

- **Python**: Scripts de scraping, limpeza e formataÃ§Ã£o dos dados.
- **AWS S3**: Armazenamento dos dados brutos e refinados.
- **AWS Lambda**: Aciona o job de ETL no Glue.
- **AWS Glue**: Processa e transforma os dados.
- **AWS Athena**: Permite consultas SQL sobre os dados processados.

## ğŸ— Estrutura do Projeto

```
ğŸ“‚ projeto-pipeline-bovespa
â”‚â”€â”€ ğŸ“‚ etl
â”‚   â”œï¸ï¸ transform.py    # Limpeza e processamento dos dados
â”‚â”€â”€ ğŸ“‚ scraping
â”‚   â”œï¸ï¸ scraper.py  # Coleta dados do site da B3
â”‚â”€â”€ ğŸ“‚ storage
â”‚   â”œï¸ï¸ s3_manager.py   # Upload para o S3
â”‚â”€ config.py  # ConfiguraÃ§Ãµes do projeto
â”‚â”€ main.py  # Agendamento e execuÃ§Ã£o do processo
â”‚â”€ requirements.txt  # DependÃªncias do projeto
â”‚â”€ README.md  # DocumentaÃ§Ã£o do projeto
```

## âš™ï¸ ConfiguraÃ§Ã£o e ExecuÃ§Ã£o

### 1ï¸âƒ£ **Instalar dependÃªncias**

```bash
pip install -r requirements.txt
```

### 2ï¸âƒ£ **Rodar o scraper manualmente**

```bash
python main.py
```

### 3ï¸âƒ£ **Executar a Lambda no AWS**

A funÃ§Ã£o Lambda pode ser implementada na AWS para detectar novos arquivos no **S3** e iniciar o **Glue**.

### 4ï¸âƒ£ **Consultar dados no Athena**

ApÃ³s o processamento, os dados podem ser consultados no **AWS Athena**.

## âœ… Requisitos Atendidos

- âœ”ï¸ **Requisito 1**: Scraping de dados do site da B3.
- âœ”ï¸ **Requisito 2**: Upload dos dados brutos no S3 em formato Parquet, partiÃ§Ã£o diÃ¡ria.
- âœ”ï¸ **Requisito 3**: O bucket deve acionar uma lambda, que por sua vez irÃ¡ chamar o job de ETL no glue.
- âœ”ï¸ **Requisito 4**: ImplementaÃ§Ã£o da Lambda em Python.
- âœ”ï¸ **Requisito 5**: Job Glue configurado no modo visual com transformaÃ§Ãµes obrigatÃ³rias.
- âœ”ï¸ **Requisito 6**: Dados refinados salvos em um bucket S3 chamado `refined/`.
- âœ”ï¸ **Requisito 7**: Glue Catalog usado para catalogar os dados.
- âœ”ï¸ **Requisito 8**: Dados disponÃ­veis para consulta via Athena.
---

ğŸ“Œ **Desenvolvido para o Tech Challenge - Fase 2 - Machine Learning AvanÃ§ado**ğŸ”— **Fonte de dados:** [B3 - Ãndice IBOV](https://sistemaswebb3-listados.b3.com.br/indexPage/day/IBOV?language=pt-br)
